{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"HifVorK22TX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import requests\n","import re\n","import time\n","\n","#Import standard libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from textblob import TextBlob\n","from nltk.tokenize.toktok import ToktokTokenizer\n","import re\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","import spacy\n","nlp = spacy.load('en_core_web_sm', disable=['ner'])\n","\n","\n","import nltk\n","from nltk.tokenize.toktok import ToktokTokenizer\n","import re\n","from bs4 import BeautifulSoup\n","# import contractions\n","# contractions.fix(\"you're happy now\")\n","# from contractions import CONTRACTION_MAP\n","import unicodedata\n"],"metadata":{"id":"TMcscwedoU_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !python -m spacy download en_core_web_sm"],"metadata":{"id":"l42FHlOZpAKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BnI9BKYc01Oy"},"outputs":[],"source":["\n","# headers = {\n","#     'Referer': 'https://www.rottentomatoes.com/m/notebook/reviews?type=user',\n","#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36',\n","#     'X-Requested-With': 'XMLHttpRequest',\n","# }\n","\n","# s = requests.Session()\n","        \n","# def get_reviews(url):\n","#     r = requests.get(url)\n","#     movie_id = re.findall(r'(?<=movieId\":\")(.*)(?=\",\"type)',r.text)[0]\n","\n","#     api_url = f\"https://www.rottentomatoes.com/napi/movie/{movie_id}/reviews/user\" #use reviews/userfor user reviews\n","#     # https://www.rottentomatoes.com/napi/movie/{movie_id}/criticsReviews/all\n","#     print(api_url)\n","#     payload = {\n","#         'direction': 'next',\n","#         'endCursor': '',\n","#         'startCursor': '',\n","#     }\n","    \n","#     review_data = []\n","#     count = 0\n","#     while True:\n","#         r = s.get(api_url, headers=headers, params=payload)\n","#         data = r.json()\n","\n","#         if not data['pageInfo']['hasNextPage']:\n","#             break\n","\n","#         payload['endCursor'] = data['pageInfo']['endCursor']\n","#         payload['startCursor'] = data['pageInfo']['startCursor'] if data['pageInfo'].get('startCursor') else ''\n","\n","#         review_data.extend(data['reviews'])\n","#         time.sleep(0.01)\n","#         count = count + 1\n","#         if count > 1000:\n","#           break\n","\n","#     return review_data\n","\n","# data = get_reviews('https://www.rottentomatoes.com/m/top_gun_maverick/reviews?type=user')\n","# df = pd.json_normalize(data)\n","# df.to_csv('top_gun_maverick.csv')"]},{"cell_type":"code","source":["df= pd.read_csv(\"/content/top_gun_maverick.csv\" , index_col=[0])\n","df.head()"],"metadata":{"id":"nPX8sKiKlLba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"Llxmjr-zCC4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1 = df.copy(deep = True)"],"metadata":{"id":"xNntjwk3CVDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(df1)"],"metadata":{"id":"8DLSXRl9COrh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1 = df1.drop(['displayName','isVerified','isSuperReviewer','hasSpoilers','hasProfanity','rating','timeFromCreation','user.displayName',\n","         'user.accountLink','user.realm','user.userId','displayImageUrl'],axis = 1)"],"metadata":{"id":"yUo5UcrSCxMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.head()"],"metadata":{"id":"a9S21hV2DfAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['label'] = [0 if x<3 else 1 for x in df1['score']]\n","df1.head()"],"metadata":{"id":"4y0BmsTHDi9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1 = df1.drop(['score'],axis = 1)"],"metadata":{"id":"BzhUVIR6D5o2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.head()"],"metadata":{"id":"GOoezQCTEBg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.isna().sum()"],"metadata":{"id":"VX-RoF4lqNmj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pie=df1['label'].value_counts()\n","pie"],"metadata":{"id":"8Zfo2nOBg9-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#to visualize the above information in the pie chart\n","plt.figure(figsize=(18,6))\n","plt.pie(pie,labels=['Positive','Negative'],colors=['red','yellow'])\n","#set the title name with fontsize\n","plt.title(\"The label percentage in TOP-GUN:MAVERICK\",fontsize=32)\n","plt.show()"],"metadata":{"id":"d8lzOls4hHGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"id":"xtJ1ZGPUzkVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","string.punctuation\n","\n","import nltk\n","from nltk.corpus import stopwords\n"],"metadata":{"id":"kXIHweA_JUZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create the remove the stopwords\n","stop_words_list=nltk.corpus.stopwords.words('english')\n","stop_words_list.remove('no')\n","stop_words_list.remove('not')"],"metadata":{"id":"qWGU8Rz-zslU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creat the function and remove the punctuation\n","def remove_punctuation(text):\n"," for punctuation in string.punctuation:\n","   text=text.replace(punctuation,'')\n"," return text\n","\n","def remove_special_characters(text, remove_digits=False):\n","    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","    text = re.sub(pattern, '', text)\n","    return text\n","    \n","def lemmatize_text(text):\n","    text = nlp(text)\n","    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n","    return text\n","\n","tokenizer = ToktokTokenizer()\n","stopword_list = nltk.corpus.stopwords.words('english')\n","stopword_list.remove('no')\n","stopword_list.remove('not')\n","def remove_stopwords(text, is_lower_case=False):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopword_list]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","    filtered_text = ' '.join(filtered_tokens)    \n","    return filtered_text"],"metadata":{"id":"SVBaYwLzuj3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['cleaned_review'] = df1['review'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True)\n","\n","df1['cleaned_review']=df1['review'].str.encode('ascii','ignore').str.decode('ascii')\n","\n","df1['cleaned_review']=df1['cleaned_review'].apply(remove_punctuation)\n","\n","df1['cleaned_review']=df1['cleaned_review'].apply(remove_special_characters)\n","\n","df1['cleaned_review'] = df1['cleaned_review'].apply(lemmatize_text)\n","\n","df1['cleaned_review'] = df1['cleaned_review'].apply(remove_stopwords)"],"metadata":{"id":"rvghKb8Mx0q-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","print('-------------------------------------------------')\n","\n","print('Average count of review per sentence in data is: {0:.0f}.'.format(df1['review'].count().mean()))\n","\n","print('-------------------------------------------------')\n","\n","print('Average word length [Uncleaned Data] : {0:.0f}.'.format(np.mean(df1['review'].apply(lambda x: len(x.split())))))\n","\n","print('--------------------------------------------------------')\n","\n","print('Average word length [Cleaned review data] : {0:.0f}.'.format(np.mean(df1['cleaned_review'].apply(lambda x: len(x.split())))))"],"metadata":{"id":"bWVn-VI000Oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.head()"],"metadata":{"id":"CvUJ4oi9FaUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('averaged_perceptron_tagger')"],"metadata":{"id":"geI4VVn44dXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# demo for POS tagging for sample news headline\n","sentence = str(df1.iloc[1].cleaned_review)\n","sentence_nlp = nlp(sentence)\n","\n","# POS tagging with Spacy \n","spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n","pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])\n","\n","# POS tagging with nltk\n","# nltk_pos_tagged = nltk.pos_tag(sentence.split())\n","# pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"],"metadata":{"id":"crCsPNs_MUwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textblob import TextBlob"],"metadata":{"id":"EVTVjMDV374A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute sentiment scores (polarity) and labels\n","df1['sentiment_scores']= [round(TextBlob(reviews).sentiment.polarity, 3) for reviews in df1['cleaned_review']]"],"metadata":{"id":"_PoQB5TRAf_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.head()"],"metadata":{"id":"AI1fJGEOBF4K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['sentiment_category'] = ['positive' if score > 0 else 'negative' for score in df1['sentiment_scores']]"],"metadata":{"id":"dXq7m0hxBJk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.head()"],"metadata":{"id":"XMuaIX9tB3vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.groupby(by=['sentiment_category']).describe()"],"metadata":{"id":"jdAw14YuCETH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.head()"],"metadata":{"id":"Uo84sFEPKU0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","\n","# mpl.rcParams['font.size']=12     # font size            \n","# mpl.rcParams['savefig.dpi']=100          \n","# mpl.rcParams['figure.subplot.bottom']=.1 \n","\n","\n","def show_wordcloud(data , title = None):\n","    wordcloud = WordCloud(\n","        background_color='black', # background color is set to black\n","        max_words=300, # maximum number of words\n","        max_font_size=40, # maximum font size is 40\n","        scale=3,\n","        random_state=1 # chosen at random by flipping a coin; it was heads\n","        \n","    ).generate(str(data))\n","    \n","    fig = plt.figure(1, figsize=(15, 15))\n","    plt.axis('off')\n","    if title: \n","        fig.suptitle(title, fontsize=20)\n","        fig.subplots_adjust(top=2.3)\n","\n","    plt.imshow(wordcloud)\n","    plt.show()\n","    \n","show_wordcloud(df1[\"cleaned_review\"])"],"metadata":{"id":"steIMSUYLZ-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1[df1['label']== 0].tail(15)"],"metadata":{"id":"G5dsGyotPBp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = df1['cleaned_review'].values\n","y= df1['label'].values"],"metadata":{"id":"0q6HAJGIM428"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Covert the text into araay using the TfidfVectorizer because the our machine\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"Gonb__CUR0OB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#install the model\n","vector=TfidfVectorizer(lowercase=False)\n","vector.fit(corpus)\n","#Transform the above fit the data using the TfidfVectorizer and finally print\n","X=vector.fit_transform(corpus)\n","X"],"metadata":{"id":"zrDoOgpBR5KV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#divided the data for train and test\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state = 123)"],"metadata":{"id":"6RcMjlcPR-52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Rows in train data:' , X_train.shape[0])\n","print('Dimensions in train data:' , X_train.shape[1])\n","print('Rows in Test data:' , X_test.shape[0])\n","print('Dimesnions in Test data:' , X_test.shape[1])"],"metadata":{"id":"yCPbqSeXSus8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import f1_score, accuracy_score"],"metadata":{"id":"LIKYEzL0Y6Om"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = []\n","\n","models.append(('Logistic Regression',LogisticRegression()))\n","models.append(('Random Forest Classifier',RandomForestClassifier()))\n","models.append(('Decision Tree Classifier',DecisionTreeClassifier()))"],"metadata":{"id":"T04SkW4-Y4xg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Engineer - Takes care of the data which data scienstst / Data Analyst Needed - Inside SQL (Bunch of quries) - will provide you the required columns\n","\n","##--> Based on these columns ---> You perform your analysis ---> Data Integrity (DeepCheck) --> What is happening in your data ---> Missing Values, Duplicate Values , Anomalies --> Columns Abusurd Values\n","# --- > Your data should make sense ---> Dig Deep Into Categorical Columns and Numerical Columns(Descrete) --- > Be Ready detailed Analysis on DATA , Good EDA , \n","\n","## ML Engineer --> They know things end-to-end ---> They have expertese in deployment on Cloud Services ---> How to create a CI/CD pipeline\n","\n","# Excellency in python  XXXX ---> Try to understand and try write your own codes ---> Python"],"metadata":{"id":"VvNu_MKSViDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for name, model in models:\n","\n","    %time model.fit(X_train, y_train)\n","    test_pred = model.predict(X_test)\n","    print(name ,'Accuracy Score : ',accuracy_score(y_test, test_pred))\n","    print(name ,'F1 Score : ',f1_score(y_test, test_pred, average='weighted'))\n","    print('-----------------------------------------------------------------------')"],"metadata":{"id":"Fc64VCKYY4vJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Topic Modelling\n"],"metadata":{"id":"Hlnu1_4kf9Q8"}},{"cell_type":"code","source":["import sys\n","# !{sys.executable} -m spacy download en\n","import re, numpy as np, pandas as pd\n","from pprint import pprint\n","\n","# Gensim\n","import gensim, spacy, logging, warnings\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","import matplotlib.pyplot as plt\n","\n","# NLTK Stop words\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])"],"metadata":{"id":"Lw_j_0I4Y4sk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sent_to_words(sentences):\n","    for sent in sentences:\n","        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n","        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n","        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n","        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n","        yield(sent)"],"metadata":{"id":"Z53f-e0EY4pU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to list\n","data = df1.review.values.tolist()\n","data_words = list(sent_to_words(data))\n","print(data_words[:1])"],"metadata":{"id":"ixWPWxhiY4hq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Build the Bigram, Trigram Models and Lemmatize"],"metadata":{"id":"Ya033kdSgXrz"}},{"cell_type":"code","source":[],"metadata":{"id":"3L0LtRMIX8JR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n","bigram_mod = gensim.models.phrases.Phraser(bigram)\n","trigram_mod = gensim.models.phrases.Phraser(trigram)\n","\n","# !python3 -m spacy download en  # run in terminal once\n","def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n","    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n","    texts = [bigram_mod[doc] for doc in texts]\n","    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n","    texts_out = []\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    #nlp = spacy.load('en', disable=['parser', 'ner'])\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","    # remove stopwords once more after lemmatization\n","    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n","    return texts_out\n","\n","data_ready = process_words(data_words)  # processed Text Data!"],"metadata":{"id":"I5H5ZKXcgYu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create Dictionary\n","id2word = corpora.Dictionary(data_ready)\n","\n","# Create Corpus: Term Document Frequency\n","corpus = [id2word.doc2bow(text) for text in data_ready]\n","\n","# Build LDA model\n","lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=4, \n","                                           random_state=100,\n","                                           update_every=1,\n","                                           chunksize=10,\n","                                           passes=10,\n","                                           alpha='symmetric',\n","                                           iterations=100,\n","                                           per_word_topics=True)\n","\n","pprint(lda_model.print_topics())"],"metadata":{"id":"kPA9W0kmgrUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n","    # Init output\n","    sent_topics_df = pd.DataFrame()\n","\n","    # Get main topic in each document\n","    for i, row_list in enumerate(ldamodel[corpus]):\n","        row = row_list[0] if ldamodel.per_word_topics else row_list            \n","        # print(row)\n","        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n","        # Get the Dominant topic, Perc Contribution and Keywords for each document\n","        for j, (topic_num, prop_topic) in enumerate(row):\n","            if j == 0:  # => dominant topic\n","                wp = ldamodel.show_topic(topic_num)\n","                topic_keywords = \", \".join([word for word, prop in wp])\n","                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n","            else:\n","                break\n","    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n","\n","    # Add original text to the end of the output\n","    contents = pd.Series(texts)\n","    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n","    return(sent_topics_df)"],"metadata":{"id":"91Ov_lPCg_Zr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_topic = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)"],"metadata":{"id":"8SDNwWXkhF6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_topic.head()"],"metadata":{"id":"iNGEl-Xhh_tv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_dominant_topic = df_topic.reset_index()\n","df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n","df_dominant_topic.head(5)"],"metadata":{"id":"ArWi4KlliwXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display setting to show more characters in column\n","pd.options.display.max_colwidth = 100\n","\n","sent_topics_sorteddf_mallet = pd.DataFrame()\n","sent_topics_outdf_grpd = df_topic.groupby('Dominant_Topic')\n","\n","for i, grp in sent_topics_outdf_grpd:\n","    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n","                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n","                                            axis=0)\n","\n","# Reset Index    \n","sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n","\n","# Format\n","sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n","\n","# Show\n","sent_topics_sorteddf_mallet.head(10)"],"metadata":{"id":"nCLbzSsPkHup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Wordcloud of Top N words in each topic\n","from matplotlib import pyplot as plt\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.colors as mcolors\n","\n","cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n","\n","cloud = WordCloud(stopwords=stop_words,\n","                  background_color='white',\n","                  width=2500,\n","                  height=1800,\n","                  max_words=10,\n","                  colormap='tab10',\n","                  color_func=lambda *args, **kwargs: cols[i],\n","                  prefer_horizontal=1.0)\n","\n","topics = lda_model.show_topics(formatted=False)\n","\n","fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n","\n","for i, ax in enumerate(axes.flatten()):\n","    fig.add_subplot(ax)\n","    topic_words = dict(topics[i][1])\n","    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n","    plt.gca().imshow(cloud)\n","    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n","    plt.gca().axis('off')\n","\n","\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.axis('off')\n","plt.margins(x=0, y=0)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"1qE8C8B_kNha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","topics = lda_model.show_topics(formatted=False)\n","data_flat = [w for w_list in data_ready for w in w_list]\n","counter = Counter(data_flat)\n","\n","out = []\n","for i, topic in topics:\n","    for word, weight in topic:\n","        out.append([word, i , weight, counter[word]])\n","\n","df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n","\n","# Plot Word Count and Weights of Topic Keywords\n","fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n","cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n","for i, ax in enumerate(axes.flatten()):\n","    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n","    ax_twin = ax.twinx()\n","    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n","    ax.set_ylabel('Word Count', color=cols[i])\n","    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n","    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n","    ax.tick_params(axis='y', left=False)\n","    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n","    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n","\n","fig.tight_layout(w_pad=2)    \n","fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n","plt.show()"],"metadata":{"id":"D2Aq9Vumk41f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install pyLDAvis"],"metadata":{"id":"jY320VfblNVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","pyLDAvis.enable_notebook()\n","vis = gensimvis.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n","vis"],"metadata":{"id":"HtKJWLColIZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MAPnKTucnabi"},"execution_count":null,"outputs":[]}]}